{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cdiscount-CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import some packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50 as CNN\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input, Flatten\n",
    "import keras.backend as K\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import threading\n",
    "from utilities import utils\n",
    "from utilities.BSONIterator import BSONIterator\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "utils.set_results_reproducible()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Lookup Tables\n",
    "First load the lookup tables from the CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/numpy/lib/arraysetops.py:463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "categories_df = pd.read_csv(\"inputs/categories.csv\", encoding = \"ISO-8859-1\", index_col=0)\n",
    "cat2idx, idx2cat = utils.make_category_tables(categories_df)\n",
    "\n",
    "train_offsets_df = pd.read_csv(\"inputs/train_offsets.csv\", encoding = \"ISO-8859-1\", index_col=0)\n",
    "train_images_df = pd.read_csv(\"inputs/train_images.csv\", encoding = \"ISO-8859-1\", index_col=0)\n",
    "val_images_df = pd.read_csv(\"inputs/val_images.csv\", encoding = \"ISO-8859-1\", index_col=0)\n",
    "\n",
    "#test_offsets_df = pd.read_csv(\"inputs/test_offsets.csv\", index_col=0)\n",
    "#test_images_df = pd.read_csv(\"inputs/test_images.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Some Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = \"inputs/\"\n",
    "\n",
    "train_bson_path = os.path.join(data_dir, \"train.bson\")\n",
    "num_train_products = 7069896\n",
    "\n",
    "#train_bson_path = \"inputs/train_example.bson\"\n",
    "#num_train_products = 82\n",
    "\n",
    "#test_bson_path = os.path.join(data_dir, \"test.bson\")\n",
    "#num_test_products = 1768172"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9901521 images belonging to 5270 classes.\n",
      "Found 2469772 images belonging to 5270 classes.\n"
     ]
    }
   ],
   "source": [
    "train_bson_file = open(train_bson_path, \"rb\")\n",
    "\n",
    "num_classes = 5270\n",
    "batch_size = 96\n",
    "input_size = 197 #180\n",
    "num_train_images = len(train_images_df)\n",
    "num_val_images = len(val_images_df)\n",
    "lock = threading.Lock()\n",
    "\n",
    "# Tip: use ImageDataGenerator for data augmentation and preprocessing.\n",
    "train_datagen = ImageDataGenerator(rotation_range=10, width_shift_range=0.1, \n",
    "       height_shift_range=0.1, shear_range=0.15, zoom_range=0.1, \n",
    "       channel_shift_range=10., horizontal_flip=True)\n",
    "train_gen = BSONIterator(train_bson_file, train_images_df, train_offsets_df, num_classes, \n",
    "                         train_datagen, lock, target_size=(input_size, input_size),\n",
    "                         batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_datagen = ImageDataGenerator()\n",
    "val_gen = BSONIterator(train_bson_file, val_images_df, train_offsets_df, num_classes,\n",
    "                       val_datagen, lock, target_size=(input_size, input_size),\n",
    "                       batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = CNN(include_top=False, input_shape=(input_size, input_size, 3), weights=None)\n",
    "model.load_weights('weights/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5')\n",
    "flatten_output = Flatten()(model.output)\n",
    "classifier = Dense(num_classes, activation='softmax')(flatten_output)\n",
    "\n",
    "model = Model(inputs=model.input, outputs=classifier)\n",
    "\n",
    "\n",
    "model.compile(optimizer=\"adam\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first layer of conv4_x (28th trainable layer from top including fc) --> refer to ResNet50 structure\n",
    "index_layer = -97 \n",
    "\n",
    "for layer in model.layers[:index_layer]: layer.trainable=False\n",
    "for layer in model.layers[index_layer:]: layer.trainable=True    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting run \"ResNet50--2017-10-10-0013\"\n",
      "Epoch 1/50\n",
      "103140/103140 [==============================] - 89019s - loss: 2.0081 - acc: 0.5829 - val_loss: 1.8398 - val_acc: 0.6313\n",
      "Epoch 2/50\n",
      "103140/103140 [==============================] - 89295s - loss: 1.8682 - acc: 0.6081 - val_loss: 1.8212 - val_acc: 0.6334\n",
      "Epoch 3/50\n",
      " 43659/103140 [===========>..................] - ETA: 47409s - loss: 1.8177 - acc: 0.6164"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-53af571f5f82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_gen\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mvalidation_steps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_val_images\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                     workers = 8)\n\u001b[0m",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     85\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     86\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 87\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   2045\u001b[0m                     outs = self.train_on_batch(x, y,\n\u001b[1;32m   2046\u001b[0m                                                \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2047\u001b[0;31m                                                class_weight=class_weight)\n\u001b[0m\u001b[1;32m   2048\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2049\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1765\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1766\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1767\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1768\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1769\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2332\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[1;32m   2333\u001b[0m                               \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2334\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2335\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/dariush/anaconda3/envs/deep1/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "\n",
    "run_name = utils.get_run_name('weights/{}.hdf5', 'ResNet50-')\n",
    "weights_path = 'weights/{}.hdf5'.format(run_name)\n",
    "\n",
    "callbacks = [EarlyStopping(monitor='val_acc',\n",
    "                           patience=3,\n",
    "                           verbose=1,\n",
    "                           min_delta=1e-4,\n",
    "                           mode='max'),\n",
    "             ReduceLROnPlateau(monitor='val_acc',\n",
    "                               factor=0.1,\n",
    "                               patience=2,\n",
    "                               verbose=1,\n",
    "                               epsilon=1e-4,\n",
    "                               mode='max'),\n",
    "             ModelCheckpoint(monitor='val_acc',\n",
    "                             filepath=weights_path,\n",
    "                             save_best_only=True,\n",
    "                             save_weights_only=True,\n",
    "                             mode='max'),\n",
    "             TensorBoard(log_dir='logs/{}'.format(run_name), batch_size=batch_size)]\n",
    "\n",
    "model.load_weights(\"weights/ResNet50-last_FC-2017-10-07-1127.hdf5\")\n",
    "K.set_value(model.optimizer.lr, 1e-4)\n",
    "\n",
    "# To train the model:\n",
    "print('Starting run \"{}\"'.format(run_name))\n",
    "model.fit_generator(train_gen,\n",
    "                    steps_per_epoch = num_train_images // batch_size,\n",
    "                    epochs = epochs,\n",
    "                    verbose=1,\n",
    "                    callbacks=callbacks,\n",
    "                    validation_data = val_gen,\n",
    "                    validation_steps = num_val_images // batch_size,\n",
    "                    workers = 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_bson_file = open(test_bson_path, \"rb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator()\n",
    "test_gen = BSONIterator(test_bson_file, test_images_df, test_offsets_df,\n",
    "                        num_classes, test_datagen, batch_size=batch_size, \n",
    "                        with_labels=False, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Running model.predict_generator() gives a list of 3095080 predictions, one for each image.\n",
    "\n",
    "The indices of the predictions correspond to the indices in test_images_df. After making the predictions, you probably want to average the predictions for products that have multiple images.\n",
    "\n",
    "Use idx2cat[] to convert the predicted category index back to the original class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_test_samples = len(test_images_df)\n",
    "predictions = model.predict_generator(test_gen, steps=num_test_samples // batch_size, workers=8)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Ah yes, the code to generate the predictions needs a lot of RAM. I only tested it on a small portion of the test set and did not realize it would take so much memory. I actually use a different way to do it now, which is to predict a mini-batch per product, so the mini batch contains either 1, 2, 3 or 4 images, and then only store the category ID, not the full prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some people mentioned earlier that they got errors on bson.BSON.decode(). I was not able to reproduce this until just now. It turns out that using ImageDataGenerator with certain data augmentation options causes this to happen.\n",
    "\n",
    "If you use rotation_range, width_shift_range, height_shift_range, shear_range, or zoom_range, then the BSON decoding gets messed up for some reason. I don't understand why but all these augmentation options use a transformation matrix, and so maybe the code that applies this matrix to the image has a bug and overwrites memory. But that is just a guess.\n",
    "\n",
    "Interestingly enough, the data augmentation options that do not use this transformation matrix, such as channel_shift_range and horizontal_flip work just fine..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Some handy codes."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# To evaluate on the validation set:\n",
    "model.evaluate_generator(val_gen, steps=num_val_images // batch_size, workers=8)\n",
    "\n",
    "pred = model.predict_generator(train_gen, steps=num_train_images/batch_size, workers=8, verbose=1)\n",
    "utils.save_array('test.hdf5', pred)\n",
    "\n",
    "new_pred = new_model.predict_generator(train_gen, steps=num_train_images/batch_size, verbose=1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# How fast is the generator? Create a single batch:\n",
    "# CPU times: user 172 ms, sys: 108 ms, total: 280 ms\n",
    "# Wall time: 381 ms\n",
    "\n",
    "next(train_gen)  # warm-up\n",
    "%time bx, by = next(train_gen)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# Does it really output images and one-hot encoded class labels?\n",
    "# Note that the images are pre-processed (and augmented) and therefore may look weird.\n",
    "\n",
    "plt.imshow(bx[-1].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "cat_idx = np.argmax(by[-1])\n",
    "cat_id = idx2cat[cat_idx]\n",
    "categories_df.loc[cat_id]\n",
    "\n",
    "#category_level1    DECO - LINGE - LUMINAIRE\n",
    "#category_level2           COUSSIN ET HOUSSE\n",
    "#category_level3                     COUSSIN\n",
    "#category_idx                           1666\n",
    "#Name: 1000001703, dtype: object"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "# CPU times: user 172 ms, sys: 12 ms, total: 184 ms\n",
    "# Wall time: 203 ms\n",
    "\n",
    "%time bx, by = next(val_gen)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "plt.imshow(bx[-1].astype(np.uint8))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "hidden": true
   },
   "source": [
    "cat_idx = np.argmax(by[-1])\n",
    "cat_id = idx2cat[cat_idx]\n",
    "categories_df.loc[cat_id]\n",
    "\n",
    "#category_level1                      TELEPHONIE - GPS\n",
    "#category_level2                  ACCESSOIRE TELEPHONE\n",
    "#category_level3    COQUE TELEPHONE - BUMPER TELEPHONE\n",
    "#category_idx                                     5055\n",
    "#Name: 1000010653, dtype: object"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
